{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bed2cd17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in ./.venv/lib/python3.11/site-packages (4.4.2)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from datasets) (3.20.2)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.11/site-packages (from datasets) (2.4.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in ./.venv/lib/python3.11/site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in ./.venv/lib/python3.11/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.11/site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./.venv/lib/python3.11/site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in ./.venv/lib/python3.11/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./.venv/lib/python3.11/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in ./.venv/lib/python3.11/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in ./.venv/lib/python3.11/site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in ./.venv/lib/python3.11/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in ./.venv/lib/python3.11/site-packages (from datasets) (0.36.0)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.11/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.11/site-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./.venv/lib/python3.11/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.3)\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.11/site-packages (from httpx<1.0.0->datasets) (4.12.1)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.11/site-packages (from httpx<1.0.0->datasets) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.11/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in ./.venv/lib/python3.11/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2.6.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.11/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.11/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.11/site-packages (from pandas->datasets) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Install (run once)\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1512e6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexmusyoka/Projects/Gen and Collab AI/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded!\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['line_text', 'filename_text', 'text', 'shorter_abstract', 'abstract'],\n",
      "        num_rows: 26147\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['line_text', 'filename_text', 'text', 'shorter_abstract', 'abstract'],\n",
      "        num_rows: 3269\n",
      "    })\n",
      "    dev: Dataset({\n",
      "        features: ['line_text', 'filename_text', 'text', 'shorter_abstract', 'abstract'],\n",
      "        num_rows: 3268\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load the SUMPUBMED dataset\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"Blaise-g/SumPubmed\")\n",
    "print(\"Dataset loaded!\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d65be1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAPER TEXT (first 500 chars):\n",
      " thioredoxins are widely distributed in nature from prokaryotes to eukaryotes. these proteins, which belong to the oxidoreductase thiol:disulfide superfamily, are characterized by the active site signature sequence wcxxc. this sequence motif constitutes the redox center mediating the isomerization of specific disulfide bridges on trx target proteins. in yeasts and mammals, the cytoplasmic trx redox system is complemented by a second trx system within mitochondria. in plants, the system is more i\n",
      "\n",
      "==================================================\n",
      "\n",
      "ABSTRACT:\n",
      " natrxh, a thioredoxin type h, shows differential expression between self-incompatible and self-compatible nicotiana species. natrxh interacts in vitro with s-rnase and co-localizes with it in the extracellular matrix of the stylar transmitting tissue. natrxh contains n- and c-terminal extensions, a feature shared by thioredoxin h proteins of subgroup to ascertain the function of these extensions in natrxh secretion and protein-protein interaction, we performed a deletion analysis on natrxh and fused the resulting variants to gfp.  we found an internal domain in the n-terminal extension, called nβ, that is essential for natrxh secretion but is not hydrophobic, a canonical feature of a signal peptide. the lack of hydrophobicity as well as the location of the secretion signal within the natrxh primary structure, suggest an unorthodox secretion route for natrxh. notably, we found that the fusion protein natrxh-gfp is retained in the endoplasmic reticulum and that treatment of natrxh-gfp-expressing cells with brefeldin a leads to its retention in the golgi, which indicates that natrxh uses, to some extent, the endoplasmic reticulum and golgi apparatus for secretion. furthermore, we found that nβ contributes to natrxh tertiary structure stabilization and that the c-terminus functions in the protein-protein interaction with s-rnase.  the extensions contained in natrxh sequence have specific functions on the protein. while the c-terminus directly participates in protein-protein interaction, particularly on its interaction with s-rnase in vitro; the n-terminal extension contains two structurally different motifs: nα and nβ. nβ, the inner domain, is essential and enough to target natrxh towards the apoplast. interestingly, when it was fused to gfp, this protein was also found in the cell wall of the onion cells. although the biochemical features of the n-terminus suggested a non-classical secretion pathway, our results provided evidence that natrxh at least uses the endoplasmic reticulum, golgi apparatus and also vesicles for secretion. therefore, the nβ domain sequence is suggested to be a novel signal peptide. thioredoxinsecretionself-incompatibilitynicotiana alatagametophytics-rnase \n"
     ]
    }
   ],
   "source": [
    "# Look at one example\n",
    "example = dataset[\"train\"][0]\n",
    "print(\"PAPER TEXT (first 500 chars):\")\n",
    "print(example[\"text\"][:500])\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "print(\"ABSTRACT:\")\n",
    "print(example[\"abstract\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5ebba73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./.venv/lib/python3.11/site-packages (2.9.1)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from torch) (3.20.2)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.11/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.11/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in ./.venv/lib/python3.11/site-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.11/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in ./.venv/lib/python3.11/site-packages (from torch) (2025.10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.11/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from jinja2->torch) (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "# Install PyTorch (required for running models)\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ed50ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.40.0 in ./.venv/lib/python3.11/site-packages (4.40.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from transformers==4.40.0) (3.20.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in ./.venv/lib/python3.11/site-packages (from transformers==4.40.0) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.11/site-packages (from transformers==4.40.0) (2.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.11/site-packages (from transformers==4.40.0) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.11/site-packages (from transformers==4.40.0) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.11/site-packages (from transformers==4.40.0) (2025.11.3)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.11/site-packages (from transformers==4.40.0) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in ./.venv/lib/python3.11/site-packages (from transformers==4.40.0) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.venv/lib/python3.11/site-packages (from transformers==4.40.0) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.11/site-packages (from transformers==4.40.0) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.0) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.0) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.0) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests->transformers==4.40.0) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.11/site-packages (from requests->transformers==4.40.0) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests->transformers==4.40.0) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests->transformers==4.40.0) (2026.1.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.40.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f5bf982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexmusyoka/Projects/Gen and Collab AI/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "Model loaded!\n",
      "Model: gpt2\n",
      "Parameters: 124,439,808\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "# Load pre-trained GPT-2\n",
    "model_name = \"gpt2\"\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "print(\"Loading model...\")\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "# GPT-2 doesn't have a pad token by default, set it to eos_token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "print(\"Model loaded!\")\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "346d0244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "INPUT (first 500 chars):\n",
      " evolutionary molecular biology is mostly concerned with the forces affecting individual genes. however, observations of variable proportions of guanine and cytosine in different species and in different genomic regions of vertebrates have prompted the analysis of forces that may affect the evolution of complete genomes. one particular hypothesis concerns adaptation to high temperatures, proposing that high gc content results from selection favouring g:c pairs over less stable a:t pairs. against\n",
      "\n",
      "============================================================\n",
      "\n",
      "GROUND TRUTH ABSTRACT:\n",
      " among bacteria and archaea, amino acid usage is correlated with habitat temperatures. in particular, protein surfaces in species thriving at higher temperatures appear to be enriched in amino acids that stabilize protein structure and depleted in amino acids that decrease thermostability. does this observation reflect a causal relationship, or could the apparent trend be caused by phylogenetic relatedness among sampled organisms living at different temperatures? and do proteins from endothermic...\n",
      "\n",
      "============================================================\n",
      "\n",
      "BASELINE GENERATED SUMMARY:\n",
      "In the present study, we show that gc is a major determinant of the evolution and survival of vertebrate genomes. We also show that a high-gc gc density is associated with a high rate of evolution in the vertebrate genome. We show that high-gc gc densities are associated with an increase in the rate of evolutionary change in the human genome, and that this increase is accompanied by a decrease in the number of genes that are involved in the development of the human organism. We conclude that the high-Gc density hypothesis is a plausible explanation for the high rates of evolution observed in human genomes.\n",
      "\n",
      "Acknowledgements: We would like to thank the following individuals for their helpful comments and suggestions:\n"
     ]
    }
   ],
   "source": [
    "# Task 3: Generate Baseline Summaries\n",
    "# Test on a single example first\n",
    "def generate_summary(text, prompt_template=\"Summarize the following medical research:\\n\\n{text}\\n\\nSummary:\", max_new_tokens=150):\n",
    "    \"\"\"Generate a summary using pre-trained GPT-2.\"\"\"\n",
    "    # Truncate text to fit within GPT-2's context window (1024 tokens)\n",
    "    # Use conservative char limit since medical text has complex tokens\n",
    "    max_input_chars = 1500\n",
    "    truncated_text = text[:max_input_chars]\n",
    "    \n",
    "    # Format prompt\n",
    "    prompt = prompt_template.format(text=truncated_text)\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=800  # Leave room for generation\n",
    "    )\n",
    "    \n",
    "    output = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        num_beams=4,\n",
    "        early_stopping=True,\n",
    "        no_repeat_ngram_size=3,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=False\n",
    "    )\n",
    "    \n",
    "    # Decode and extract only the generated part (after the prompt)\n",
    "    full_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    summary = full_output[len(prompt):].strip()\n",
    "    \n",
    "    return summary\n",
    "# Test on first test example\n",
    "test_example = dataset[\"test\"][0]\n",
    "print(\"=\" * 60)\n",
    "print(\"INPUT (first 500 chars):\")\n",
    "print(test_example[\"text\"][:500])\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\\nGROUND TRUTH ABSTRACT:\")\n",
    "print(test_example[\"abstract\"][:500] + \"...\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\\nBASELINE GENERATED SUMMARY:\")\n",
    "baseline_summary = generate_summary(test_example[\"text\"])\n",
    "print(baseline_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9970a929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BASELINE CONFIGURATION\n",
      "============================================================\n",
      "Model: gpt2\n",
      "Parameters: 124M\n",
      "Transformers version: 4.40.0\n",
      "Max input tokens: 800\n",
      "Max new tokens: 150\n",
      "Num beams: 4\n",
      "No repeat ngram size: 3\n",
      "Do sample: False (deterministic)\n",
      "============================================================\n",
      "\n",
      "GENERATING BASELINE SUMMARIES...\n",
      "\n",
      "============================================================\n",
      "OUTPUT 1 | PROMPT_1 | Sample index 0\n",
      "============================================================\n",
      "\n",
      "PROMPT: Summarize this text:\n",
      "\n",
      "{text}\n",
      "\n",
      "Summary:...\n",
      "\n",
      "INPUT (first 300 chars):\n",
      " evolutionary molecular biology is mostly concerned with the forces affecting individual genes. however, observations of variable proportions of guanine and cytosine in different species and in different genomic regions of vertebrates have prompted the analysis of forces that may affect the evolutio...\n",
      "\n",
      "GROUND TRUTH (first 200 chars):\n",
      " among bacteria and archaea, amino acid usage is correlated with habitat temperatures. in particular, protein surfaces in species thriving at higher temperatures appear to be enriched in amino acids t...\n",
      "\n",
      "GENERATED SUMMARY:\n",
      "In this paper, we show that gc is an important determinant of the evolutionary history of vertebrate genomes. We also show that the evolution and evolution of complex proteins is influenced by gc, and that this influence is not limited to gc alone, but also to other factors such as gc abundance. We show that a high-gc gc density is associated with a high rate of evolution in the vertebrate genome, which is consistent with the hypothesis that the high-gc density is a result of selection on gc.\n",
      "\n",
      "Acknowledgments\n",
      "\n",
      "The authors would like to thank the following individuals for their helpful comments and suggestions:\n",
      "....\n",
      "\n",
      "J. M. Molloy, M.\n",
      "\n",
      "============================================================\n",
      "OUTPUT 2 | PROMPT_1 | Sample index 100\n",
      "============================================================\n",
      "\n",
      "PROMPT: Summarize this text:\n",
      "\n",
      "{text}\n",
      "\n",
      "Summary:...\n",
      "\n",
      "INPUT (first 300 chars):\n",
      " the increasing availability of pathway information enables the comparative analyses of organisms at the functional level. as the basis for such studies we propose to use pathway alignment, an effective technique that provides both a similarity score as well as a clear indication of specific differe...\n",
      "\n",
      "GROUND TRUTH (first 200 chars):\n",
      " pathways provide topical descriptions of cellular circuitry. comparing analogous pathways reveals intricate insights into individual functional differences among species. while previous works in the ...\n",
      "\n",
      "GENERATED SUMMARY:\n",
      "In this paper, we present the results of a comparative study of the evolution of the metabolic pathways of different species. The results of this comparative study are presented as a set of phylogenetic and evolutionary data. The phylogenetic data are based on the phylogenetic tree of the species, and the evolutionary data are derived from phylogenetic trees of other organisms. The evolutionary data can be compared to the evolutionary tree of other species, or to the tree of all other species.\n",
      "\n",
      "Acknowledgments\n",
      "\n",
      "This work was supported by a grant from the National Science Foundation (grant No. R01-R01-01) and the National Institute of Environmental Health Sciences (grants No. S01-S01-0009 and S01\n",
      "\n",
      "============================================================\n",
      "OUTPUT 3 | PROMPT_2 | Sample index 500\n",
      "============================================================\n",
      "\n",
      "PROMPT: Summarize the following medical research:\n",
      "\n",
      "{text}\n",
      "\n",
      "Summary:...\n",
      "\n",
      "INPUT (first 300 chars):\n",
      " \"candidatus phytoplasma aurantifolia\" is an obligate biotrophic plant pathogen that causes witches' broom disease in mexican lime trees. this is a devastating disease that results in significant economic losses. phytoplasmas are prokaryotes that inhabit the phloem and are transmitted by phloem-suck...\n",
      "\n",
      "GROUND TRUTH (first 200 chars):\n",
      " \"candidatus phytoplasma aurantifolia\", is the causative agent of witches' broom disease in mexican lime trees, and is responsible for major losses of mexican lime trees in southern iran and oman. the...\n",
      "\n",
      "GENERATED SUMMARY:\n",
      "Phytoplosan is a highly toxic and highly toxic herbicide that has been shown to cause serious health problems in humans and animals. It has been used as a pesticide in the United States since the 1950s. It is also used as an insecticide in the European Union. It was first used in the 1970s and has been widely used in Europe since the 1990s. Phytoposan is used in a wide range of applications including insecticides, herbicides, fungicides, insect repellents, insecticides on crops, and insecticides and fungicides on plants. It can be used in combination with other herbicides to produce a wide variety of toxic and toxic herbicides. Phytosan has been found\n",
      "\n",
      "============================================================\n",
      "OUTPUT 4 | PROMPT_2 | Sample index 1000\n",
      "============================================================\n",
      "\n",
      "PROMPT: Summarize the following medical research:\n",
      "\n",
      "{text}\n",
      "\n",
      "Summary:...\n",
      "\n",
      "INPUT (first 300 chars):\n",
      " basic cellular systems including the cell cycle, innate immunity, and mrna translation operate in a similar manner across a large number of species. the proteins that participate in these systems are highly conserved, enabling many successful applications to infer gene function based on sequence si...\n",
      "\n",
      "GROUND TRUTH (first 200 chars):\n",
      " orthologous genes are highly conserved between closely related species and biological systems often utilize the same genes across different organisms. however, while sequence similarity often implies...\n",
      "\n",
      "GENERATED SUMMARY:\n",
      "In the present study, we have shown that the expression profiles of a wide variety of yeast species are similar to those of human and mice. We have also shown that a large proportion of the variation in gene expression across species is due to differences in the number of genes involved in the transcriptional and mRNAs involved in cell cycle regulation. We also found that the gene expression profile of yeast is similar to that of humans and mice, and that the differences in expression between species are not due to a single gene, but rather to a number of factors.\n",
      "\n",
      "Acknowledgments\n",
      "\n",
      "This work was supported by grants from the National Institutes of Health, the National Science Foundation, and the National Institute of Allergy and Infectious Diseases.\n",
      "\n",
      "============================================================\n",
      "OUTPUT 5 | PROMPT_3 | Sample index 1500\n",
      "============================================================\n",
      "\n",
      "PROMPT: Provide a brief summary:\n",
      "\n",
      "{text}\n",
      "\n",
      "Summary:...\n",
      "\n",
      "INPUT (first 300 chars):\n",
      " transcriptional and post-transcriptional events involve the interplay between protein effectors and nucleic acid targets, whose physical interaction is guided by sequence motifs and specific structural elements. motifs are usually defined as short nucleotide sequence patterns of length k and repres...\n",
      "\n",
      "GROUND TRUTH (first 200 chars):\n",
      " the large amount of data produced by high-throughput sequencing poses new computational challenges. in the last decade, several tools have been developed for the identification of transcription and s...\n",
      "\n",
      "GENERATED SUMMARY:\n",
      "In this paper, we present the results of a systematic review and meta-analysis of the literature on the role of motifs in the regulation of gene expression and gene expression regulation in human disease. We propose a novel approach to the identification of novel regulatory elements that can be used to identify novel therapeutic targets for human diseases.\n",
      "\n",
      "Introduction\n",
      "\n",
      "The role of a motif in gene expression is well-established. In the context of human disease, it is important to understand the role that motifs play in regulating gene expression. In this review, we propose a new approach to this question: the identification and characterization of novel therapeutic target for human disease (MDR).\n",
      "\n",
      "MDR is a disease-modifying disorder that is characterized by\n",
      "\n",
      "============================================================\n",
      "OUTPUT 6 | PROMPT_3 | Sample index 2000\n",
      "============================================================\n",
      "\n",
      "PROMPT: Provide a brief summary:\n",
      "\n",
      "{text}\n",
      "\n",
      "Summary:...\n",
      "\n",
      "INPUT (first 300 chars):\n",
      " rapeseed is an important oilseed crop throughout the world, serving as source for edible oil and renewable energy. it is an amphidiploid species which originated from a few interspecific hybridizations between b. rapa and b. oleracea. this in turn led to a low genetic diversity in b. napus. the occ...\n",
      "\n",
      "GROUND TRUTH (first 200 chars):\n",
      " rapeseed is an important oilseed crop throughout the world, serving as source for edible oil and renewable energy. development of nested association mapping population and methods is of importance fo...\n",
      "\n",
      "GENERATED SUMMARY:\n",
      "In the present study, we show that a high level of genetic diversity is associated with a high degree of genetic vulnerability to selection, and that this genetic vulnerability can be reduced by introducing heterozygosity and heterozygous phenotypes. We also show that high levels of genetic variation are associated with an increased susceptibility to selection in a wide range of plant species. In addition, we demonstrate that high genetic diversity can reduce the genetic vulnerability of a wide variety of species.\n",
      "\n",
      "Acknowledgments\n",
      "\n",
      "This work was supported by grants from the National Science Foundation, the National Institute of Allergy and Infectious Diseases, and the National Institutes of Health. The funders had no role in study design, data collection and analysis, decision to publish, or\n",
      "\n",
      "============================================================\n",
      "OUTPUT 7 | PROMPT_4 | Sample index 2500\n",
      "============================================================\n",
      "\n",
      "PROMPT: TL;DR:\n",
      "\n",
      "{text}\n",
      "\n",
      "TL;DR:...\n",
      "\n",
      "INPUT (first 300 chars):\n",
      " the aanat family is part of the large and diverse superfamily of gcn5-like acetyltransferases which use accoa as the acetyl donor and share a common accoa binding fold. members of the aanat family share limited sequence identity and are divided into two groups: vertebrate aanats; and, the non-verte...\n",
      "\n",
      "GROUND TRUTH (first 200 chars):\n",
      " the arylalkylamine n-acetyltransferase family is divided into structurally distinct vertebrate and non-vertebrate groups. expression of vertebrate aanats is limited primarily to the pineal gland and ...\n",
      "\n",
      "GENERATED SUMMARY:\n",
      "aanat has been shown to be a member of the metalloprotein family of proteins, and to be involved in the regulation of circadian rhythms in vertebrates.\n",
      "\n",
      "The role of aanates in circadian rhythms is well established, but it is not clear whether they play a critical role in regulating the circadian rhythms of other vertebrates, such as mammals, birds, reptiles, and amphibians, or whether they contribute to the circadian regulation of the circadian rhythm of other species. The role of anaates in regulating circadian rhythms has been suggested to be due to their ability to regulate the circadian activity of other proteins, including melatonin and melatonin receptors, which are involved in regulating melatonin production and secretion. melatonin is\n",
      "\n",
      "============================================================\n",
      "OUTPUT 8 | PROMPT_4 | Sample index 3000\n",
      "============================================================\n",
      "\n",
      "PROMPT: TL;DR:\n",
      "\n",
      "{text}\n",
      "\n",
      "TL;DR:...\n",
      "\n",
      "INPUT (first 300 chars):\n",
      " nodals are secreted signaling proteins in the tgfβ superfamily that have many established roles in vertebrate development. the absence of nodal signaling in mice and zebrafish results in loss of the gastrula organizer, an almost complete failure in the development of mesodermal and endodermal tissu...\n",
      "\n",
      "GROUND TRUTH (first 200 chars):\n",
      " nodals are secreted signaling proteins with many roles in vertebrate development. here, we identify a new role for nodal signaling in regulating closure of the rostral neural tube of zebrafish.  we f...\n",
      "\n",
      "GENERATED SUMMARY:\n",
      "Nodal signaling is essential for the development and maintenance of the neural tube. it is also critical for the regulation of the central nervous system and for the maintenance of a healthy immune system.\n",
      "\n",
      "\n",
      "NODAL STATISTICS:\n",
      "\n",
      "\n",
      "1. Nodal signals have been shown to play a role in the formation of neural tube structures.\n",
      "\n",
      "2. In the present study, we investigated the role of norepinephrine (NE) in regulating the expression of the N-methyl-D-aspartate (NMDA) receptor (NMDAR) in the right ventricle and the left ventricular septum. We found that the NMDAR is expressed in both the right and left ventric\n",
      "\n",
      "\n",
      "============================================================\n",
      "Generated 8 baseline outputs using 4 prompts\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Task 3: Generate Baseline Summaries with Multiple Prompts\n",
    "# ==========================================================\n",
    "# Define baseline prompts\n",
    "BASELINE_PROMPTS = {\n",
    "    \"prompt_1\": \"Summarize this text:\\n\\n{text}\\n\\nSummary:\",\n",
    "    \"prompt_2\": \"Summarize the following medical research:\\n\\n{text}\\n\\nSummary:\",\n",
    "    \"prompt_3\": \"Provide a brief summary:\\n\\n{text}\\n\\nSummary:\",\n",
    "    \"prompt_4\": \"TL;DR:\\n\\n{text}\\n\\nTL;DR:\"\n",
    "}\n",
    "# Fixed sample indices for reproducibility\n",
    "sample_indices = [0, 100, 500, 1000, 1500, 2000, 2500, 3000]\n",
    "# Assign 2 samples per prompt\n",
    "prompt_sample_mapping = {\n",
    "    \"prompt_1\": [0, 100],\n",
    "    \"prompt_2\": [500, 1000],\n",
    "    \"prompt_3\": [1500, 2000],\n",
    "    \"prompt_4\": [2500, 3000]\n",
    "}\n",
    "# Document configuration\n",
    "print(\"=\" * 60)\n",
    "print(\"BASELINE CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Model: gpt2\")\n",
    "print(f\"Parameters: 124M\")\n",
    "print(f\"Transformers version: 4.40.0\")\n",
    "print(f\"Max input tokens: 800\")\n",
    "print(f\"Max new tokens: 150\")\n",
    "print(f\"Num beams: 4\")\n",
    "print(f\"No repeat ngram size: 3\")\n",
    "print(f\"Do sample: False (deterministic)\")\n",
    "print(\"=\" * 60)\n",
    "# Generate summaries\n",
    "baseline_results = []\n",
    "output_count = 0\n",
    "print(\"\\nGENERATING BASELINE SUMMARIES...\\n\")\n",
    "for prompt_name, indices in prompt_sample_mapping.items():\n",
    "    prompt_template = BASELINE_PROMPTS[prompt_name]\n",
    "    \n",
    "    for idx in indices:\n",
    "        output_count += 1\n",
    "        example = dataset[\"test\"][idx]\n",
    "        generated = generate_summary(example[\"text\"], prompt_template=prompt_template)\n",
    "        \n",
    "        baseline_results.append({\n",
    "            \"output_num\": output_count,\n",
    "            \"index\": idx,\n",
    "            \"prompt_name\": prompt_name,\n",
    "            \"prompt_template\": prompt_template,\n",
    "            \"input_text\": example[\"text\"],\n",
    "            \"ground_truth\": example[\"abstract\"],\n",
    "            \"generated\": generated\n",
    "        })\n",
    "        \n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"OUTPUT {output_count} | {prompt_name.upper()} | Sample index {idx}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"\\nPROMPT: {prompt_template[:60]}...\")\n",
    "        print(f\"\\nINPUT (first 300 chars):\\n{example['text'][:300]}...\")\n",
    "        print(f\"\\nGROUND TRUTH (first 200 chars):\\n{example['abstract'][:200]}...\")\n",
    "        print(f\"\\nGENERATED SUMMARY:\\n{generated}\")\n",
    "        print()\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Generated {len(baseline_results)} baseline outputs using {len(BASELINE_PROMPTS)} prompts\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848b487a",
   "metadata": {},
   "source": [
    "# Task 4 - Baseline Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6261c765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge-score in ./.venv/lib/python3.11/site-packages (0.1.2)\n",
      "Requirement already satisfied: bert-score in ./.venv/lib/python3.11/site-packages (0.3.13)\n",
      "Requirement already satisfied: absl-py in ./.venv/lib/python3.11/site-packages (from rouge-score) (2.3.1)\n",
      "Requirement already satisfied: nltk in ./.venv/lib/python3.11/site-packages (from rouge-score) (3.9.2)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.11/site-packages (from rouge-score) (2.4.0)\n",
      "Requirement already satisfied: six>=1.14.0 in ./.venv/lib/python3.11/site-packages (from rouge-score) (1.17.0)\n",
      "Requirement already satisfied: torch>=1.0.0 in ./.venv/lib/python3.11/site-packages (from bert-score) (2.9.1)\n",
      "Requirement already satisfied: pandas>=1.0.1 in ./.venv/lib/python3.11/site-packages (from bert-score) (2.3.3)\n",
      "Requirement already satisfied: transformers>=3.0.0 in ./.venv/lib/python3.11/site-packages (from bert-score) (4.40.0)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.11/site-packages (from bert-score) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.31.1 in ./.venv/lib/python3.11/site-packages (from bert-score) (4.67.1)\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.11/site-packages (from bert-score) (3.10.8)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.venv/lib/python3.11/site-packages (from bert-score) (25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.11/site-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.11/site-packages (from pandas>=1.0.1->bert-score) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.11/site-packages (from pandas>=1.0.1->bert-score) (2025.3)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from torch>=1.0.0->bert-score) (3.20.2)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.11/site-packages (from torch>=1.0.0->bert-score) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.11/site-packages (from torch>=1.0.0->bert-score) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in ./.venv/lib/python3.11/site-packages (from torch>=1.0.0->bert-score) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.11/site-packages (from torch>=1.0.0->bert-score) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in ./.venv/lib/python3.11/site-packages (from torch>=1.0.0->bert-score) (2025.10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=1.0.0->bert-score) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in ./.venv/lib/python3.11/site-packages (from transformers>=3.0.0->bert-score) (0.36.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.11/site-packages (from transformers>=3.0.0->bert-score) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.11/site-packages (from transformers>=3.0.0->bert-score) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in ./.venv/lib/python3.11/site-packages (from transformers>=3.0.0->bert-score) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.venv/lib/python3.11/site-packages (from transformers>=3.0.0->bert-score) (0.7.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers>=3.0.0->bert-score) (1.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.11/site-packages (from matplotlib->bert-score) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.11/site-packages (from matplotlib->bert-score) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.11/site-packages (from matplotlib->bert-score) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.11/site-packages (from matplotlib->bert-score) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.11/site-packages (from matplotlib->bert-score) (12.1.0)\n",
      "Requirement already satisfied: pyparsing>=3 in ./.venv/lib/python3.11/site-packages (from matplotlib->bert-score) (3.3.1)\n",
      "Requirement already satisfied: click in ./.venv/lib/python3.11/site-packages (from nltk->rouge-score) (8.3.1)\n",
      "Requirement already satisfied: joblib in ./.venv/lib/python3.11/site-packages (from nltk->rouge-score) (1.5.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests->bert-score) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.11/site-packages (from requests->bert-score) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests->bert-score) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests->bert-score) (2026.1.4)\n"
     ]
    }
   ],
   "source": [
    "# Install evaluation libraries\n",
    "!pip install rouge-score bert-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07789161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ROUGE EVALUATION\n",
      "============================================================\n",
      "\n",
      "Sample 1 (prompt_1): R1=0.301, R2=0.025, RL=0.150\n",
      "Sample 2 (prompt_1): R1=0.290, R2=0.061, RL=0.148\n",
      "Sample 3 (prompt_2): R1=0.186, R2=0.005, RL=0.115\n",
      "Sample 4 (prompt_2): R1=0.306, R2=0.015, RL=0.138\n",
      "Sample 5 (prompt_3): R1=0.350, R2=0.059, RL=0.162\n",
      "Sample 6 (prompt_3): R1=0.262, R2=0.025, RL=0.131\n",
      "Sample 7 (prompt_4): R1=0.297, R2=0.068, RL=0.171\n",
      "Sample 8 (prompt_4): R1=0.333, R2=0.091, RL=0.209\n",
      "\n",
      "------------------------------------------------------------\n",
      "ROUGE AVERAGES:\n",
      "  ROUGE-1: 0.2907\n",
      "  ROUGE-2: 0.0437\n",
      "  ROUGE-L: 0.1529\n"
     ]
    }
   ],
   "source": [
    "# Task 4: Evaluate Baseline - ROUGE Scores\n",
    "# =========================================\n",
    "from rouge_score import rouge_scorer\n",
    "import numpy as np\n",
    "# Initialize ROUGE scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "# Store scores\n",
    "rouge1_scores = []\n",
    "rouge2_scores = []\n",
    "rougeL_scores = []\n",
    "print(\"=\" * 60)\n",
    "print(\"ROUGE EVALUATION\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "for result in baseline_results:\n",
    "    scores = scorer.score(result[\"ground_truth\"], result[\"generated\"])\n",
    "    \n",
    "    rouge1_scores.append(scores[\"rouge1\"].fmeasure)\n",
    "    rouge2_scores.append(scores[\"rouge2\"].fmeasure)\n",
    "    rougeL_scores.append(scores[\"rougeL\"].fmeasure)\n",
    "    \n",
    "    print(f\"Sample {result['output_num']} ({result['prompt_name']}): \"\n",
    "          f\"R1={scores['rouge1'].fmeasure:.3f}, \"\n",
    "          f\"R2={scores['rouge2'].fmeasure:.3f}, \"\n",
    "          f\"RL={scores['rougeL'].fmeasure:.3f}\")\n",
    "# Averages\n",
    "avg_rouge1 = np.mean(rouge1_scores)\n",
    "avg_rouge2 = np.mean(rouge2_scores)\n",
    "avg_rougeL = np.mean(rougeL_scores)\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"ROUGE AVERAGES:\")\n",
    "print(f\"  ROUGE-1: {avg_rouge1:.4f}\")\n",
    "print(f\"  ROUGE-2: {avg_rouge2:.4f}\")\n",
    "print(f\"  ROUGE-L: {avg_rougeL:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cdfcdfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BERTSCORE EVALUATION\n",
      "============================================================\n",
      "\n",
      "Calculating BERTScore (this may take a minute)...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexmusyoka/Projects/Gen and Collab AI/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:05<00:00,  5.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 100.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 5.06 seconds, 1.58 sentences/sec\n",
      "Sample 1 (prompt_1): P=0.833, R=0.809, F1=0.821\n",
      "Sample 2 (prompt_1): P=0.816, R=0.816, F1=0.816\n",
      "Sample 3 (prompt_2): P=0.814, R=0.770, F1=0.792\n",
      "Sample 4 (prompt_2): P=0.833, R=0.812, F1=0.822\n",
      "Sample 5 (prompt_3): P=0.845, R=0.802, F1=0.823\n",
      "Sample 6 (prompt_3): P=0.813, R=0.775, F1=0.793\n",
      "Sample 7 (prompt_4): P=0.864, R=0.802, F1=0.832\n",
      "Sample 8 (prompt_4): P=0.834, R=0.817, F1=0.825\n",
      "\n",
      "------------------------------------------------------------\n",
      "BERTSCORE AVERAGES:\n",
      "  Precision: 0.8315\n",
      "  Recall:    0.8004\n",
      "  F1:        0.8156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Task 4: Evaluate Baseline - BERTScore\n",
    "# =====================================\n",
    "from bert_score import score as bert_score\n",
    "print(\"=\" * 60)\n",
    "print(\"BERTSCORE EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nCalculating BERTScore (this may take a minute)...\\n\")\n",
    "generated_texts = [r[\"generated\"] for r in baseline_results]\n",
    "reference_texts = [r[\"ground_truth\"] for r in baseline_results]\n",
    "P, R, F1 = bert_score(generated_texts, reference_texts, lang=\"en\", verbose=True)\n",
    "# Individual scores\n",
    "for i, result in enumerate(baseline_results):\n",
    "    print(f\"Sample {result['output_num']} ({result['prompt_name']}): \"\n",
    "          f\"P={P[i]:.3f}, R={R[i]:.3f}, F1={F1[i]:.3f}\")\n",
    "# Averages\n",
    "avg_bert_p = P.mean().item()\n",
    "avg_bert_r = R.mean().item()\n",
    "avg_bert_f1 = F1.mean().item()\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"BERTSCORE AVERAGES:\")\n",
    "print(f\"  Precision: {avg_bert_p:.4f}\")\n",
    "print(f\"  Recall:    {avg_bert_r:.4f}\")\n",
    "print(f\"  F1:        {avg_bert_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9c91d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BASELINE EVALUATION SUMMARY\n",
      "============================================================\n",
      "\n",
      "┌─────────────────────────────────────────┐\n",
      "│  BASELINE RESULTS                       │\n",
      "├─────────────────────────────────────────┤\n",
      "│  ROUGE-1:        0.2907                   │\n",
      "│  ROUGE-2:        0.0437                   │\n",
      "│  ROUGE-L:        0.1529                   │\n",
      "│  BERTScore F1:   0.8156                   │\n",
      "└─────────────────────────────────────────┘\n",
      "\n",
      "Baseline metrics stored in 'baseline_metrics' for later comparison\n"
     ]
    }
   ],
   "source": [
    "# Task 4: Baseline Evaluation Summary\n",
    "# ====================================\n",
    "print(\"=\" * 60)\n",
    "print(\"BASELINE EVALUATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\"\"\n",
    "┌─────────────────────────────────────────┐\n",
    "│  BASELINE RESULTS                       │\n",
    "├─────────────────────────────────────────┤\n",
    "│  ROUGE-1:        {avg_rouge1:.4f}                   │\n",
    "│  ROUGE-2:        {avg_rouge2:.4f}                   │\n",
    "│  ROUGE-L:        {avg_rougeL:.4f}                   │\n",
    "│  BERTScore F1:   {avg_bert_f1:.4f}                   │\n",
    "└─────────────────────────────────────────┘\n",
    "\"\"\")\n",
    "# Store for later comparison (Task 9)\n",
    "baseline_metrics = {\n",
    "    \"rouge1\": avg_rouge1,\n",
    "    \"rouge2\": avg_rouge2,\n",
    "    \"rougeL\": avg_rougeL,\n",
    "    \"bertscore_p\": avg_bert_p,\n",
    "    \"bertscore_r\": avg_bert_r,\n",
    "    \"bertscore_f1\": avg_bert_f1\n",
    "}\n",
    "print(\"Baseline metrics stored in 'baseline_metrics' for later comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d08a94",
   "metadata": {},
   "source": [
    "# TASK 5 - PROMPT Engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4bcf9f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PROMPT ENGINEERING: VERSION OVERVIEW\n",
      "============================================================\n",
      "\n",
      "V1_ROLE\n",
      "  Change: Role framing (starting point)\n",
      "  Template preview: You are a medical researcher. Summarize this research:\n",
      "\n",
      "{tex...\n",
      "\n",
      "V2_AUDIENCE\n",
      "  Change: + Audience targeting\n",
      "  Template preview: You are a medical researcher. Summarize this research for a ...\n",
      "\n",
      "V3_CONSTRAINTS\n",
      "  Change: + Constraints\n",
      "  Template preview: You are a medical researcher. Summarize this research for a ...\n",
      "\n",
      "V4_STRUCTURE\n",
      "  Change: + Structured output\n",
      "  Template preview: You are a medical researcher. Summarize this research for a ...\n",
      "\n",
      "V5_FEWSHOT\n",
      "  Change: + Few-shot example\n",
      "  Template preview: You are a medical researcher. Summarize this research for a ...\n",
      "\n",
      "============================================================\n",
      "Testing on 8 samples: [0, 100, 500, 1000, 1500, 2000, 2500, 3000]\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Task 5: Prompt Engineering - Versioned Prompts with Ablation\n",
    "# ============================================================\n",
    "PROMPT_VERSIONS = {\n",
    "    \"v1_role\": {\n",
    "        \"template\": \"You are a medical researcher. Summarize this research:\\n\\n{text}\\n\\nSummary:\",\n",
    "        \"change\": \"Role framing (starting point)\"\n",
    "    },\n",
    "    \n",
    "    \"v2_audience\": {\n",
    "        \"template\": \"You are a medical researcher. Summarize this research for a patient with no medical background:\\n\\n{text}\\n\\nSummary:\",\n",
    "        \"change\": \"+ Audience targeting\"\n",
    "    },\n",
    "    \n",
    "    \"v3_constraints\": {\n",
    "        \"template\": \"You are a medical researcher. Summarize this research for a patient with no medical background. Only include factual claims from the text. Do not include acknowledgments or references.\\n\\n{text}\\n\\nSummary:\",\n",
    "        \"change\": \"+ Constraints\"\n",
    "    },\n",
    "    \n",
    "    \"v4_structure\": {\n",
    "        \"template\": \"You are a medical researcher. Summarize this research for a patient with no medical background. Only include factual claims from the text. Do not include acknowledgments or references.\\n\\nSummarize in 3 points:\\n1) What was studied\\n2) Key findings\\n3) Implications\\n\\n{text}\\n\\nSummary:\",\n",
    "        \"change\": \"+ Structured output\"\n",
    "    },\n",
    "    \n",
    "    \"v5_fewshot\": {\n",
    "        \"template\": \"\"\"You are a medical researcher. Summarize this research for a patient with no medical background. Only include factual claims from the text. Do not include acknowledgments or references.\n",
    "Example:\n",
    "Text: \"This study examined the effects of vitamin D supplementation on bone density in elderly patients. 200 participants received either vitamin D or placebo for 12 months. Results showed a 15% improvement in bone density in the treatment group.\"\n",
    "Summary: 1) Studied vitamin D supplements and bone health in elderly patients. 2) Found 15% improvement in bone density with vitamin D. 3) Suggests vitamin D may help prevent bone loss in older adults.\n",
    "Now summarize in 3 points:\n",
    "{text}\n",
    "Summary:\"\"\",\n",
    "        \"change\": \"+ Few-shot example\"\n",
    "    }\n",
    "}\n",
    "# Same indices as baseline for fair comparison\n",
    "sample_indices = [0, 100, 500, 1000, 1500, 2000, 2500, 3000]\n",
    "# Print summary\n",
    "print(\"=\" * 60)\n",
    "print(\"PROMPT ENGINEERING: VERSION OVERVIEW\")\n",
    "print(\"=\" * 60)\n",
    "for version, info in PROMPT_VERSIONS.items():\n",
    "    print(f\"\\n{version.upper()}\")\n",
    "    print(f\"  Change: {info['change']}\")\n",
    "    print(f\"  Template preview: {info['template'][:60]}...\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"Testing on {len(sample_indices)} samples: {sample_indices}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32b2151a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GENERATING SUMMARIES FOR EACH PROMPT VERSION\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Processing: V1_ROLE\n",
      "Change: Role framing (starting point)\n",
      "============================================================\n",
      "\n",
      "Sample 1 (index 0):\n",
      "Generated: In this paper, we present the results of a large-scale comparative study of the effects of high temperature and low gc on the development of a complet...\n",
      "\n",
      "Sample 2 (index 100):\n",
      "Generated: In this paper, we present the results of a large-scale phylogenetic analysis of a wide range of pathogenic organisms. We show that the phylogenetic re...\n",
      "\n",
      "Sample 3 (index 500):\n",
      "Generated: Phytoplosan is a highly toxic and highly toxic plant organism. It has been shown that it can cause serious health problems in humans and animals. It i...\n",
      "\n",
      "Sample 4 (index 1000):\n",
      "Generated: In this paper, we present the results of a large genome-wide association study (GWAS) of the human genome. The results of the study are summarized in ...\n",
      "\n",
      "Sample 5 (index 1500):\n",
      "Generated: In this paper, we present the results of a systematic review and meta-analysis of genome-wide association studies (GWASs) that have been conducted in ...\n",
      "\n",
      "Sample 6 (index 2000):\n",
      "Generated: In the present study, we examined the association between rapeeed and a number of phenotypes. We found that rapeeed was associated with a wide range o...\n",
      "\n",
      "Sample 7 (index 2500):\n",
      "Generated: aanat genes are expressed in vertebrates, but not in other vertebrates.\n",
      "\n",
      "Aanat DNA is expressed in many vertebrate species, but only in a few vertebra...\n",
      "\n",
      "Sample 8 (index 3000):\n",
      "Generated: Nodal signaling is an important part of the development and development of the brain. It is also involved in the regulation of the central nervous sys...\n",
      "\n",
      " v1_role: Generated 8 summaries\n",
      "\n",
      "============================================================\n",
      "Processing: V2_AUDIENCE\n",
      "Change: + Audience targeting\n",
      "============================================================\n",
      "\n",
      "Sample 1 (index 0):\n",
      "Generated: In this paper, we present the results of a large-scale comparative study of the effects of high temperature and low gc on the evolution and evolution ...\n",
      "\n",
      "Sample 2 (index 100):\n",
      "Generated: In this paper, we present the results of a large-scale phylogenetic analysis of a wide range of organisms. We show that the phylogenetic relationships...\n",
      "\n",
      "Sample 3 (index 500):\n",
      "Generated: Phytoplosan is a highly toxic plant-pathogen that has been shown to cause serious health problems in humans and animals. It is also known to cause liv...\n",
      "\n",
      "Sample 4 (index 1000):\n",
      "Generated: In this paper, we present the results of a genome-wide association study (GWAS) of the human genome. The results of the study are summarized in Table ...\n",
      "\n",
      "Sample 5 (index 1500):\n",
      "Generated: In this paper, we present the results of a systematic review and meta-analysis of the literature on the role of transcription factors in the regulatio...\n",
      "\n",
      "Sample 6 (index 2000):\n",
      "Generated: In the present study, we investigated the association between rapeseed seed and the risk of developing a sexually transmitted infection (STI). We foun...\n",
      "\n",
      "Sample 7 (index 2500):\n",
      "Generated: aanat genes are expressed in a wide range of tissues and organs, including the brain, heart, lungs, liver, kidneys, pancreas, intestines, and skeletal...\n",
      "\n",
      "Sample 8 (index 3000):\n",
      "Generated: Nodal signaling is an important part of the development and maintenance of the brain and spinal cord, and it plays a critical role in the function of ...\n",
      "\n",
      " v2_audience: Generated 8 summaries\n",
      "\n",
      "============================================================\n",
      "Processing: V3_CONSTRAINTS\n",
      "Change: + Constraints\n",
      "============================================================\n",
      "\n",
      "Sample 1 (index 0):\n",
      "Generated: In this paper, we present the results of a systematic review and meta-analysis of the literature on the effects of high temperature on the development...\n",
      "\n",
      "Sample 2 (index 100):\n",
      "Generated: In this paper, we present the results of a large-scale phylogenetic analysis of a wide range of pathogenic organisms. We show that the phylogenetic re...\n",
      "\n",
      "Sample 3 (index 500):\n",
      "Generated: Phytoplosporin is a phytoestrogen that has been shown to be effective in the treatment of psoriasis and psoriatic dermatitis. It is also known to be a...\n",
      "\n",
      "Sample 4 (index 1000):\n",
      "Generated: In this paper, we present the results of a large genome-wide association study (GWAS) of the human genome. The results of the study are summarized in ...\n",
      "\n",
      "Sample 5 (index 1500):\n",
      "Generated: In this paper, we present the results of a systematic review and meta-analysis of the literature on the role of motifs in the regulation of gene expre...\n",
      "\n",
      "Sample 6 (index 2000):\n",
      "Generated: In the present study, we examined the association between genetic variation and phenotypes in a wide range of plant species. We found that genetic var...\n",
      "\n",
      "Sample 7 (index 2500):\n",
      "Generated: aanat genes are expressed in vertebrates, but they are not found in mammals, birds, reptiles, amphibians, and mammals.\n",
      "\n",
      "Aanat Genes\n",
      "\n",
      "1.1. Aanat DNA\n",
      "\n",
      "I...\n",
      "\n",
      "Sample 8 (index 3000):\n",
      "Generated: Nodal signaling has been shown to play a major role in the regulation of the development and development of vertebrate vertebrates. However, it is not...\n",
      "\n",
      " v3_constraints: Generated 8 summaries\n",
      "\n",
      "============================================================\n",
      "Processing: V4_STRUCTURE\n",
      "Change: + Structured output\n",
      "============================================================\n",
      "\n",
      "Sample 1 (index 0):\n",
      "Generated: In this paper, we present the results of a large-scale comparative study of the effects of high temperature and low gc on the development of a complet...\n",
      "\n",
      "Sample 2 (index 100):\n",
      "Generated: In this paper, we present the results of a large-scale comparative analysis of a wide range of organisms. We show that a wide variety of pathways are ...\n",
      "\n",
      "Sample 3 (index 500):\n",
      "Generated: Phytoplosporin is a phytoestrogen that has been shown to be effective in the treatment of psoriasis and psoriatic ulcerative colitis. It is also known...\n",
      "\n",
      "Sample 4 (index 1000):\n",
      "Generated: In this paper, we present the results of a large genome-wide association study (GWAS) of the human genome. The study was conducted at the National Ins...\n",
      "\n",
      "Sample 5 (index 1500):\n",
      "Generated: In this paper, we present the results of a systematic review and meta-analysis of genome-wide association studies (GWASs) that have been conducted in ...\n",
      "\n",
      "Sample 6 (index 2000):\n",
      "Generated: In this paper, we present the results of a genome-wide association study (GWAS) that has been used to identify genetic variants associated with rapeee...\n",
      "\n",
      "Sample 7 (index 2500):\n",
      "Generated: aanat has been identified as a member of the metalloprotein family of acetyl-transferases, a group of enzymes that are involved in acetylation of sero...\n",
      "\n",
      "Sample 8 (index 3000):\n",
      "Generated: Nodal signaling is an important part of the development and function of the brain. It is also involved in the regulation of the body's immune system, ...\n",
      "\n",
      " v4_structure: Generated 8 summaries\n",
      "\n",
      "============================================================\n",
      "Processing: V5_FEWSHOT\n",
      "Change: + Few-shot example\n",
      "============================================================\n",
      "\n",
      "Sample 1 (index 0):\n",
      "Generated: 2) A study of the effect of a vitamin D supplement on bone mineral density in a group of elderly subjects.\n",
      "Conclusion: 3) A review of the literature o...\n",
      "\n",
      "Sample 2 (index 100):\n",
      "Generated: 2) In this paper, we present the results of a large-scale comparative study of the effect of dietary vitamin D on bone mass and bone mineral density i...\n",
      "\n",
      "Sample 3 (index 500):\n",
      "Generated: 2) Finds that vitamin D is beneficial for bone health. 3. Suggests it may be beneficial for people with osteoporosis. 4. Suggested that it may also be...\n",
      "\n",
      "Sample 4 (index 1000):\n",
      "Generated: 1) This study found a significant association between vitamin D and bone mineral density.\n",
      "2) Vitamin D supplementation was associated with an increase...\n",
      "\n",
      "Sample 5 (index 1500):\n",
      "Generated: 2) Determines the relevance of a given motif to a given gene. 3. Evaluates whether a particular motif is associated with a specific gene. 4. Estimates...\n",
      "\n",
      "Sample 6 (index 2000):\n",
      "Generated: 2) Finds that high levels of dietary vitamin D are associated with a lower risk of osteoporosis in older women.\n",
      "In summary: 3) Provides evidence that ...\n",
      "\n",
      "Sample 7 (index 2500):\n",
      "Generated: The aanate family is a group of genes that are expressed in a wide range of tissues and organs, including the brain, heart, lungs, liver, kidneys, pan...\n",
      "\n",
      "Sample 8 (index 3000):\n",
      "Generated: 2) Nodal signaling is involved in the regulation of the right ventricle and the left ventricles. 3. Nodals are involved in both the right and left ven...\n",
      "\n",
      " v5_fewshot: Generated 8 summaries\n",
      "\n",
      "============================================================\n",
      " All 5 prompt versions processed\n",
      " Results stored in 'prompt_engineering_results'\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Task 5: Generate Summaries for Each Prompt Version\n",
    "# ===================================================\n",
    "prompt_engineering_results = {}\n",
    "print(\"=\" * 60)\n",
    "print(\"GENERATING SUMMARIES FOR EACH PROMPT VERSION\")\n",
    "print(\"=\" * 60)\n",
    "for version_name, version_info in PROMPT_VERSIONS.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing: {version_name.upper()}\")\n",
    "    print(f\"Change: {version_info['change']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    version_results = []\n",
    "    \n",
    "    for i, idx in enumerate(sample_indices):\n",
    "        example = dataset[\"test\"][idx]\n",
    "        generated = generate_summary(\n",
    "            example[\"text\"], \n",
    "            prompt_template=version_info[\"template\"]\n",
    "        )\n",
    "        \n",
    "        version_results.append({\n",
    "            \"index\": idx,\n",
    "            \"ground_truth\": example[\"abstract\"],\n",
    "            \"generated\": generated\n",
    "        })\n",
    "        \n",
    "        print(f\"\\nSample {i+1} (index {idx}):\")\n",
    "        print(f\"Generated: {generated[:150]}...\")\n",
    "    \n",
    "    prompt_engineering_results[version_name] = version_results\n",
    "    print(f\"\\n {version_name}: Generated {len(version_results)} summaries\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\" All {len(PROMPT_VERSIONS)} prompt versions processed\")\n",
    "print(f\" Results stored in 'prompt_engineering_results'\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a067f544",
   "metadata": {},
   "source": [
    "# Evaluate All Prompt Versions (ROUGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75c92fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EVALUATING PROMPT VERSIONS - ROUGE SCORES\n",
      "============================================================\n",
      "\n",
      "V1_ROLE\n",
      "  ROUGE-1: 0.2503\n",
      "  ROUGE-2: 0.0389\n",
      "  ROUGE-L: 0.1424\n",
      "\n",
      "V2_AUDIENCE\n",
      "  ROUGE-1: 0.2588\n",
      "  ROUGE-2: 0.0360\n",
      "  ROUGE-L: 0.1442\n",
      "\n",
      "V3_CONSTRAINTS\n",
      "  ROUGE-1: 0.2608\n",
      "  ROUGE-2: 0.0305\n",
      "  ROUGE-L: 0.1392\n",
      "\n",
      "V4_STRUCTURE\n",
      "  ROUGE-1: 0.2917\n",
      "  ROUGE-2: 0.0416\n",
      "  ROUGE-L: 0.1539\n",
      "\n",
      "V5_FEWSHOT\n",
      "  ROUGE-1: 0.1966\n",
      "  ROUGE-2: 0.0194\n",
      "  ROUGE-L: 0.1229\n",
      "\n",
      "============================================================\n",
      "ROUGE evaluation complete\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Task 5: Evaluate Prompt Versions - ROUGE Scores\n",
    "# ================================================\n",
    "from rouge_score import rouge_scorer\n",
    "import numpy as np\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "# Store metrics for each version\n",
    "prompt_engineering_metrics = {}\n",
    "print(\"=\" * 60)\n",
    "print(\"EVALUATING PROMPT VERSIONS - ROUGE SCORES\")\n",
    "print(\"=\" * 60)\n",
    "for version_name, results in prompt_engineering_results.items():\n",
    "    rouge1_scores = []\n",
    "    rouge2_scores = []\n",
    "    rougeL_scores = []\n",
    "    \n",
    "    for result in results:\n",
    "        scores = scorer.score(result[\"ground_truth\"], result[\"generated\"])\n",
    "        rouge1_scores.append(scores[\"rouge1\"].fmeasure)\n",
    "        rouge2_scores.append(scores[\"rouge2\"].fmeasure)\n",
    "        rougeL_scores.append(scores[\"rougeL\"].fmeasure)\n",
    "    \n",
    "    prompt_engineering_metrics[version_name] = {\n",
    "        \"rouge1\": np.mean(rouge1_scores),\n",
    "        \"rouge2\": np.mean(rouge2_scores),\n",
    "        \"rougeL\": np.mean(rougeL_scores)\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{version_name.upper()}\")\n",
    "    print(f\"  ROUGE-1: {np.mean(rouge1_scores):.4f}\")\n",
    "    print(f\"  ROUGE-2: {np.mean(rouge2_scores):.4f}\")\n",
    "    print(f\"  ROUGE-L: {np.mean(rougeL_scores):.4f}\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ROUGE evaluation complete\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e62d18f",
   "metadata": {},
   "source": [
    "# Evaluate All Prompt Versions (BERTScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f412bd59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EVALUATING PROMPT VERSIONS - BERTSCORE\n",
      "============================================================\n",
      "\n",
      "This may take a few minutes...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V1_ROLE: BERTScore F1 = 0.8071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V2_AUDIENCE: BERTScore F1 = 0.8102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V3_CONSTRAINTS: BERTScore F1 = 0.8122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V4_STRUCTURE: BERTScore F1 = 0.8167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V5_FEWSHOT: BERTScore F1 = 0.7932\n",
      "\n",
      "============================================================\n",
      "BERTScore evaluation complete\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Task 5: Evaluate Prompt Versions - BERTScore\n",
    "# =============================================\n",
    "from bert_score import score as bert_score\n",
    "print(\"=\" * 60)\n",
    "print(\"EVALUATING PROMPT VERSIONS - BERTSCORE\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nThis may take a few minutes...\\n\")\n",
    "for version_name, results in prompt_engineering_results.items():\n",
    "    generated_texts = [r[\"generated\"] for r in results]\n",
    "    reference_texts = [r[\"ground_truth\"] for r in results]\n",
    "    \n",
    "    P, R, F1 = bert_score(generated_texts, reference_texts, lang=\"en\", verbose=False)\n",
    "    \n",
    "    prompt_engineering_metrics[version_name][\"bertscore_p\"] = P.mean().item()\n",
    "    prompt_engineering_metrics[version_name][\"bertscore_r\"] = R.mean().item()\n",
    "    prompt_engineering_metrics[version_name][\"bertscore_f1\"] = F1.mean().item()\n",
    "    \n",
    "    print(f\"{version_name.upper()}: BERTScore F1 = {F1.mean().item():.4f}\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"BERTScore evaluation complete\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd97759",
   "metadata": {},
   "source": [
    " # Before/After Comparison Grid (Ablation Table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "691df4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ABLATION ANALYSIS: BEFORE/AFTER GRID\n",
      "============================================================\n",
      "\n",
      "Version         Change                         R-1      R-2      R-L      BERT-F1  Δ R-L   \n",
      "-----------------------------------------------------------------------------------------------\n",
      "Baseline        (Task 3 best)                  0.2907   0.0437   0.1529   0.8156   --      \n",
      "v1_role         Role framing (starting point)  0.2503   0.0389   0.1424   0.8071   -0.0105 \n",
      "v2_audience     + Audience targeting           0.2588   0.0360   0.1442   0.8102   +0.0019 \n",
      "v3_constraints  + Constraints                  0.2608   0.0305   0.1392   0.8122   -0.0050 \n",
      "v4_structure    + Structured output            0.2917   0.0416   0.1539   0.8167   +0.0146 \n",
      "v5_fewshot      + Few-shot example             0.1966   0.0194   0.1229   0.7932   -0.0310 \n",
      "-----------------------------------------------------------------------------------------------\n",
      "\n",
      "BEST VERSION:   v4_structure\n",
      "TOTAL Δ R-L:    +0.0010 (from 0.1529 to 0.1539)\n",
      "IMPROVEMENT:    0.6%\n",
      "\n",
      "============================================================\n",
      "Ablation analysis complete\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Task 5: Ablation Analysis - Before/After Grid\n",
    "# ==============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"ABLATION ANALYSIS: BEFORE/AFTER GRID\")\n",
    "print(\"=\" * 60)\n",
    "# Header\n",
    "print(f\"\\n{'Version':<15} {'Change':<30} {'R-1':<8} {'R-2':<8} {'R-L':<8} {'BERT-F1':<8} {'Δ R-L':<8}\")\n",
    "print(\"-\" * 95)\n",
    "# Baseline reference\n",
    "baseline_rougeL = baseline_metrics[\"rougeL\"]\n",
    "print(f\"{'Baseline':<15} {'(Task 3 best)':<30} {baseline_metrics['rouge1']:.4f}   {baseline_metrics['rouge2']:.4f}   {baseline_rougeL:.4f}   {baseline_metrics['bertscore_f1']:.4f}   {'--':<8}\")\n",
    "# Previous version for delta calculation\n",
    "prev_rougeL = baseline_rougeL\n",
    "# Each prompt engineering version\n",
    "for version_name, version_info in PROMPT_VERSIONS.items():\n",
    "    metrics = prompt_engineering_metrics[version_name]\n",
    "    delta = metrics[\"rougeL\"] - prev_rougeL\n",
    "    delta_str = f\"{delta:+.4f}\"\n",
    "    \n",
    "    print(f\"{version_name:<15} {version_info['change']:<30} {metrics['rouge1']:.4f}   {metrics['rouge2']:.4f}   {metrics['rougeL']:.4f}   {metrics['bertscore_f1']:.4f}   {delta_str:<8}\")\n",
    "    \n",
    "    prev_rougeL = metrics[\"rougeL\"]\n",
    "print(\"-\" * 95)\n",
    "# Overall improvement\n",
    "best_version = max(prompt_engineering_metrics.keys(), key=lambda x: prompt_engineering_metrics[x][\"rougeL\"])\n",
    "best_rougeL = prompt_engineering_metrics[best_version][\"rougeL\"]\n",
    "total_improvement = best_rougeL - baseline_rougeL\n",
    "print(f\"\\n{'BEST VERSION:':<15} {best_version}\")\n",
    "print(f\"{'TOTAL Δ R-L:':<15} {total_improvement:+.4f} (from {baseline_rougeL:.4f} to {best_rougeL:.4f})\")\n",
    "print(f\"{'IMPROVEMENT:':<15} {(total_improvement/baseline_rougeL)*100:.1f}%\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Ablation analysis complete\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3d99bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEMPERATURE VARIATION TEST (using v4_structure)\n",
      "============================================================\n",
      "\n",
      "Testing temperature = 0.0...\n",
      "  Temperature 0.0: ROUGE-L = 0.1010\n",
      "\n",
      "Testing temperature = 0.3...\n",
      "  Temperature 0.3: ROUGE-L = 0.1513\n",
      "\n",
      "Testing temperature = 0.7...\n",
      "  Temperature 0.7: ROUGE-L = 0.1396\n",
      "\n",
      "Testing temperature = 1.0...\n",
      "  Temperature 1.0: ROUGE-L = 0.1223\n",
      "\n",
      "============================================================\n",
      "TEMPERATURE COMPARISON\n",
      "============================================================\n",
      "  temp=0.0: 0.1010\n",
      "  temp=0.3: 0.1513\n",
      "  temp=0.7: 0.1396\n",
      "  temp=1.0: 0.1223\n"
     ]
    }
   ],
   "source": [
    "# Task 5: Temperature Variation Test\n",
    "# ===================================\n",
    "# Test on best performing prompt (v4_structure)\n",
    "TEMPERATURE_VALUES = [0.0, 0.3, 0.7, 1.0]\n",
    "temperature_results = {}\n",
    "# Use v4_structure template (best performer)\n",
    "best_template = PROMPT_VERSIONS[\"v4_structure\"][\"template\"]\n",
    "print(\"=\" * 60)\n",
    "print(\"TEMPERATURE VARIATION TEST (using v4_structure)\")\n",
    "print(\"=\" * 60)\n",
    "for temp in TEMPERATURE_VALUES:\n",
    "    print(f\"\\nTesting temperature = {temp}...\")\n",
    "    \n",
    "    temp_outputs = []\n",
    "    for idx in sample_indices[:4]:  # Test on first 4 samples for speed\n",
    "        example = dataset[\"test\"][idx]\n",
    "        \n",
    "        # Modify generate_summary to use temperature\n",
    "        truncated_text = example[\"text\"][:1500]\n",
    "        prompt = best_template.format(text=truncated_text)\n",
    "        \n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=800)\n",
    "        \n",
    "        output = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_new_tokens=150,\n",
    "            num_beams=1,  # Beam search doesn't work with temperature\n",
    "            do_sample=True if temp > 0 else False,\n",
    "            temperature=temp if temp > 0 else 1.0,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "        full_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        generated = full_output[len(prompt):].strip()\n",
    "        temp_outputs.append({\"generated\": generated, \"ground_truth\": example[\"abstract\"]})\n",
    "    \n",
    "    # Calculate ROUGE\n",
    "    scores = [scorer.score(r[\"ground_truth\"], r[\"generated\"])[\"rougeL\"].fmeasure for r in temp_outputs]\n",
    "    avg_rougeL = np.mean(scores)\n",
    "    \n",
    "    temperature_results[temp] = avg_rougeL\n",
    "    print(f\"  Temperature {temp}: ROUGE-L = {avg_rougeL:.4f}\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TEMPERATURE COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "for temp, score in temperature_results.items():\n",
    "    print(f\"  temp={temp}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebfb554",
   "metadata": {},
   "source": [
    "# Further Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "851d6863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TESTING: CONSTRAINTS ONLY (Non-Stacked)\n",
      "============================================================\n",
      "\n",
      "Constraints Only ROUGE-L: 0.1423\n",
      "Baseline ROUGE-L:         0.1529\n",
      "v3_constraints (stacked): 0.1392\n",
      "\n",
      "Difference vs baseline:   -0.0106\n"
     ]
    }
   ],
   "source": [
    "# Experiment: Non-Stacked Prompt - Constraints Only\n",
    "# ==================================================\n",
    "# Testing if constraints ALONE perform better than stacked prompts\n",
    "# (removes v1_role which hurt performance)\n",
    "constraints_only_template = \"Only include factual claims from the text. Do not include acknowledgments or references.\\n\\n{text}\\n\\nSummary:\"\n",
    "print(\"=\"*60)\n",
    "print(\"TESTING: CONSTRAINTS ONLY (Non-Stacked)\")\n",
    "print(\"=\"*60)\n",
    "constraints_only_results = []\n",
    "for idx in sample_indices:\n",
    "    example = dataset[\"test\"][idx]\n",
    "    generated = generate_summary(example[\"text\"], prompt_template=constraints_only_template)\n",
    "    \n",
    "    constraints_only_results.append({\n",
    "        \"index\": idx,\n",
    "        \"ground_truth\": example[\"abstract\"],\n",
    "        \"generated\": generated\n",
    "    })\n",
    "# Evaluate\n",
    "rouge_scores = []\n",
    "for result in constraints_only_results:\n",
    "    scores = scorer.score(result[\"ground_truth\"], result[\"generated\"])\n",
    "    rouge_scores.append(scores[\"rougeL\"].fmeasure)\n",
    "avg_rougeL = np.mean(rouge_scores)\n",
    "print(f\"\\nConstraints Only ROUGE-L: {avg_rougeL:.4f}\")\n",
    "print(f\"Baseline ROUGE-L:         {baseline_metrics['rougeL']:.4f}\")\n",
    "print(f\"v3_constraints (stacked): {prompt_engineering_metrics['v3_constraints']['rougeL']:.4f}\")\n",
    "print(f\"\\nDifference vs baseline:   {avg_rougeL - baseline_metrics['rougeL']:+.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8bd13858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TESTING: STRUCTURE ONLY (Non-Stacked)\n",
      "============================================================\n",
      "\n",
      "Structure Only ROUGE-L:   0.1472\n",
      "Baseline ROUGE-L:         0.1529\n",
      "v4_structure (stacked):   0.1539\n",
      "\n",
      "Difference vs baseline:   -0.0058\n"
     ]
    }
   ],
   "source": [
    "# Experiment: Non-Stacked Prompt - Structure Only\n",
    "# ================================================\n",
    "# Testing if structured output format helps on its own\n",
    "structure_only_template = \"Summarize in 3 points:\\n1) What was studied\\n2) Key findings\\n3) Implications\\n\\n{text}\\n\\nSummary:\"\n",
    "print(\"=\"*60)\n",
    "print(\"TESTING: STRUCTURE ONLY (Non-Stacked)\")\n",
    "print(\"=\"*60)\n",
    "structure_only_results = []\n",
    "for idx in sample_indices:\n",
    "    example = dataset[\"test\"][idx]\n",
    "    generated = generate_summary(example[\"text\"], prompt_template=structure_only_template)\n",
    "    \n",
    "    structure_only_results.append({\n",
    "        \"index\": idx,\n",
    "        \"ground_truth\": example[\"abstract\"],\n",
    "        \"generated\": generated\n",
    "    })\n",
    "# Evaluate\n",
    "rouge_scores = []\n",
    "for result in structure_only_results:\n",
    "    scores = scorer.score(result[\"ground_truth\"], result[\"generated\"])\n",
    "    rouge_scores.append(scores[\"rougeL\"].fmeasure)\n",
    "avg_rougeL = np.mean(rouge_scores)\n",
    "print(f\"\\nStructure Only ROUGE-L:   {avg_rougeL:.4f}\")\n",
    "print(f\"Baseline ROUGE-L:         {baseline_metrics['rougeL']:.4f}\")\n",
    "print(f\"v4_structure (stacked):   {prompt_engineering_metrics['v4_structure']['rougeL']:.4f}\")\n",
    "print(f\"\\nDifference vs baseline:   {avg_rougeL - baseline_metrics['rougeL']:+.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task7_header",
   "metadata": {},
   "source": [
    "# TASK 7 - Fine-Tune Generator with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "install_peft",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: peft==0.11.0 in ./.venv/lib/python3.11/site-packages (0.11.0)\n",
      "Requirement already satisfied: accelerate in ./.venv/lib/python3.11/site-packages (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.11/site-packages (from peft==0.11.0) (2.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.11/site-packages (from peft==0.11.0) (25.0)\n",
      "Requirement already satisfied: psutil in ./.venv/lib/python3.11/site-packages (from peft==0.11.0) (7.2.1)\n",
      "Requirement already satisfied: pyyaml in ./.venv/lib/python3.11/site-packages (from peft==0.11.0) (6.0.3)\n",
      "Requirement already satisfied: torch>=1.13.0 in ./.venv/lib/python3.11/site-packages (from peft==0.11.0) (2.9.1)\n",
      "Requirement already satisfied: transformers in ./.venv/lib/python3.11/site-packages (from peft==0.11.0) (4.40.0)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.11/site-packages (from peft==0.11.0) (4.67.1)\n",
      "Requirement already satisfied: safetensors in ./.venv/lib/python3.11/site-packages (from peft==0.11.0) (0.7.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in ./.venv/lib/python3.11/site-packages (from peft==0.11.0) (0.36.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from huggingface-hub>=0.17.0->peft==0.11.0) (3.20.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.11/site-packages (from huggingface-hub>=0.17.0->peft==0.11.0) (2025.10.0)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.11/site-packages (from huggingface-hub>=0.17.0->peft==0.11.0) (2.32.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.11/site-packages (from huggingface-hub>=0.17.0->peft==0.11.0) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.11/site-packages (from huggingface-hub>=0.17.0->peft==0.11.0) (1.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.11.0) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in ./.venv/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.11.0) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.11.0) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=1.13.0->peft==0.11.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from jinja2->torch>=1.13.0->peft==0.11.0) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.11.0) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.11/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.11.0) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.11.0) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests->huggingface-hub>=0.17.0->peft==0.11.0) (2026.1.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.11/site-packages (from transformers->peft==0.11.0) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in ./.venv/lib/python3.11/site-packages (from transformers->peft==0.11.0) (0.19.1)\n"
     ]
    }
   ],
   "source": [
    "# Install PEFT (Parameter-Efficient Fine-Tuning) library\n",
    "!pip install peft==0.11.0 accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "lora_config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LoRA CONFIGURATION\n",
      "============================================================\n",
      "Rank (r): 8\n",
      "Alpha: 32\n",
      "Dropout: 0.1\n",
      "Target modules: {'c_attn'}\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Task 7: Configure LoRA for GPT-2 Fine-Tuning\n",
    "# ============================================\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import torch\n",
    "\n",
    "# LoRA Configuration\n",
    "# Based on Hu et al. (2022) recommendations\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,  # GPT-2 is a causal language model\n",
    "    r=8,                            # Rank of low-rank matrices\n",
    "    lora_alpha=32,                  # Scaling factor\n",
    "    lora_dropout=0.1,               # Dropout for regularization\n",
    "    target_modules=[\"c_attn\"],      # GPT-2 attention projection\n",
    ")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"LoRA CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Rank (r): {lora_config.r}\")\n",
    "print(f\"Alpha: {lora_config.lora_alpha}\")\n",
    "print(f\"Dropout: {lora_config.lora_dropout}\")\n",
    "print(f\"Target modules: {lora_config.target_modules}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "apply_lora",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fresh GPT-2 model for fine-tuning...\n",
      "Applying LoRA adapters...\n",
      "\n",
      "============================================================\n",
      "PARAMETER EFFICIENCY\n",
      "============================================================\n",
      "Total parameters: 124,734,720\n",
      "Trainable parameters: 294,912\n",
      "Trainable %: 0.2364%\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexmusyoka/Projects/Gen and Collab AI/.venv/lib/python3.11/site-packages/peft/tuners/lora/layer.py:1119: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Apply LoRA to GPT-2\n",
    "# ===================\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load fresh model for fine-tuning\n",
    "print(\"Loading fresh GPT-2 model for fine-tuning...\")\n",
    "base_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "base_model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Apply LoRA\n",
    "print(\"Applying LoRA adapters...\")\n",
    "lora_model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "trainable_params = sum(p.numel() for p in lora_model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in lora_model.parameters())\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"PARAMETER EFFICIENCY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Trainable %: {100 * trainable_params / total_params:.4f}%\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "prepare_data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing training data...\n",
      "Training samples: 500\n",
      "Batches per epoch: 250\n"
     ]
    }
   ],
   "source": [
    "# Prepare Training Data\n",
    "# =====================\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SummarizationDataset(Dataset):\n",
    "    \"\"\"Dataset for summarization fine-tuning.\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_split, tokenizer, max_length=512, num_samples=500):\n",
    "        self.examples = []\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Use subset for training (laptop-friendly)\n",
    "        for i in range(min(num_samples, len(dataset_split))):\n",
    "            example = dataset_split[i]\n",
    "            # Format: input text -> summary\n",
    "            text = example[\"text\"][:1000]  # Truncate input\n",
    "            summary = example[\"abstract\"][:300]  # Truncate summary\n",
    "            \n",
    "            # Create training format\n",
    "            prompt = f\"Summarize:\\n{text}\\n\\nSummary: {summary}\"\n",
    "            self.examples.append(prompt)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.examples[idx],\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": encoding[\"input_ids\"].squeeze()  # For causal LM, labels = input_ids\n",
    "        }\n",
    "\n",
    "# Create datasets\n",
    "print(\"Preparing training data...\")\n",
    "train_dataset = SummarizationDataset(dataset[\"train\"], tokenizer, num_samples=500)\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "print(f\"Batches per epoch: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "training_loop",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FINE-TUNING CONFIGURATION\n",
      "============================================================\n",
      "Device: mps\n",
      "Epochs: 3\n",
      "Learning rate: 0.0005\n",
      "Batch size: 2\n",
      "Training samples: 500\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 250/250 [01:51<00:00,  2.24it/s, loss=1.8618]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Average Loss: 1.9764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 250/250 [01:46<00:00,  2.34it/s, loss=1.7919]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Average Loss: 1.7532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 250/250 [01:45<00:00,  2.37it/s, loss=1.7109]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Average Loss: 1.7284\n",
      "\n",
      "============================================================\n",
      "TRAINING COMPLETE\n",
      "============================================================\n",
      "Final loss: 1.7284\n",
      "Loss reduction: 0.2480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Task 7: Fine-Tune with LoRA\n",
    "# ===========================\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Training configuration\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 5e-4\n",
    "DEVICE = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"FINE-TUNING CONFIGURATION\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Epochs: {EPOCHS}\")\n",
    "print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"Batch size: 2\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Move model to device\n",
    "lora_model = lora_model.to(DEVICE)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(lora_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Training loop\n",
    "lora_model.train()\n",
    "training_losses = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        # Move batch to device\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        labels = batch[\"labels\"].to(DEVICE)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = lora_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "    \n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    training_losses.append(avg_loss)\n",
    "    print(f\"Epoch {epoch+1} Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Final loss: {training_losses[-1]:.4f}\")\n",
    "print(f\"Loss reduction: {training_losses[0] - training_losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "save_model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA adapters saved to: ./lora_gpt2_medical\n",
      "Files saved: ['adapter_model.safetensors', 'README.md', 'adapter_config.json']\n"
     ]
    }
   ],
   "source": [
    "# Save the fine-tuned LoRA adapters\n",
    "# =================================\n",
    "import os\n",
    "\n",
    "output_dir = \"./lora_gpt2_medical\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "lora_model.save_pretrained(output_dir)\n",
    "print(f\"LoRA adapters saved to: {output_dir}\")\n",
    "print(f\"Files saved: {os.listdir(output_dir)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task8_header",
   "metadata": {},
   "source": [
    "# TASK 8 - Evaluate Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "generate_finetuned",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GENERATING FINE-TUNED SUMMARIES\n",
      "============================================================\n",
      "\n",
      "Sample 1 (index 0):\n",
      "Generated: high-gc content in vertebrates has been proposed as a possible explanation for the evolutionary divergence between vertebrates and other vertebrates. ...\n",
      "\n",
      "Sample 2 (index 100):\n",
      "Generated: we have developed a method that allows for the comparative analysis of evolutionary changes among species. this method is based on the concept of a ph...\n",
      "\n",
      "Sample 3 (index 500):\n",
      "Generated: the phytophthora infestans is one of the most common pathogenic fungi in the world. the pathogen has been implicated in the pathogenesis of many disea...\n",
      "\n",
      "Sample 4 (index 1000):\n",
      "Generated: genome-wide association studies have been conducted to investigate the relationship between gene expression and gene function. in this study, we exami...\n",
      "\n",
      "Sample 5 (index 1500):\n",
      "Generated: genome-wide association studies (GWAS) are an important tool for the identification of novel regulatory elements. the aim of the present study was to ...\n",
      "\n",
      "Sample 6 (index 2000):\n",
      "Generated: rapeseed has been a major source of food for many plant species and has been used as a food source for many plants. however, it has been difficult to ...\n",
      "\n",
      "Sample 7 (index 2500):\n",
      "Generated: vertebrate acetyl transferases are part of a broad class of superfamily that includes vertebrates, fungi, bacteria, and protists. vertebrates are the ...\n",
      "\n",
      "Sample 8 (index 3000):\n",
      "Generated: nodal signalling in vertebrates has been implicated in the regulation of vertebrate growth and development. however, the role of nodals in mesoderma d...\n",
      "\n",
      "============================================================\n",
      "Generated 8 summaries\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Task 8: Generate Summaries with Fine-Tuned Model\n",
    "# =================================================\n",
    "\n",
    "def generate_summary_finetuned(text, model, tokenizer, max_new_tokens=150):\n",
    "    \"\"\"Generate summary using fine-tuned model.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Use same format as training\n",
    "    prompt = f\"Summarize:\\n{text[:1000]}\\n\\nSummary:\"\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=800\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            num_beams=4,\n",
    "            early_stopping=True,\n",
    "            no_repeat_ngram_size=3,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=False\n",
    "        )\n",
    "    \n",
    "    full_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    # Extract only the summary part\n",
    "    if \"Summary:\" in full_output:\n",
    "        summary = full_output.split(\"Summary:\")[-1].strip()\n",
    "    else:\n",
    "        summary = full_output[len(prompt):].strip()\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Generate summaries on test set\n",
    "print(\"=\"*60)\n",
    "print(\"GENERATING FINE-TUNED SUMMARIES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "finetuned_results = []\n",
    "sample_indices = [0, 100, 500, 1000, 1500, 2000, 2500, 3000]  # Same as baseline\n",
    "\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    example = dataset[\"test\"][idx]\n",
    "    generated = generate_summary_finetuned(example[\"text\"], lora_model, tokenizer)\n",
    "    \n",
    "    finetuned_results.append({\n",
    "        \"index\": idx,\n",
    "        \"ground_truth\": example[\"abstract\"],\n",
    "        \"generated\": generated\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nSample {i+1} (index {idx}):\")\n",
    "    print(f\"Generated: {generated[:150]}...\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Generated {len(finetuned_results)} summaries\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "evaluate_finetuned",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FINE-TUNED MODEL EVALUATION\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ROUGE-1: 0.1938\n",
      "ROUGE-2: 0.0370\n",
      "ROUGE-L: 0.1188\n",
      "BERTScore F1: 0.8289\n",
      "\n",
      "============================================================\n",
      "IMPROVEMENT OVER BASELINE\n",
      "============================================================\n",
      "ROUGE-L: 0.1529 -> 0.1188 (-22.3%)\n",
      "BERTScore: 0.8156 -> 0.8289 (+1.6%)\n"
     ]
    }
   ],
   "source": [
    "# Task 8: Evaluate Fine-Tuned Model\n",
    "# =================================\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bert_score\n",
    "import numpy as np\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "# Calculate ROUGE\n",
    "print(\"=\"*60)\n",
    "print(\"FINE-TUNED MODEL EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "rouge1_scores = []\n",
    "rouge2_scores = []\n",
    "rougeL_scores = []\n",
    "\n",
    "for result in finetuned_results:\n",
    "    scores = scorer.score(result[\"ground_truth\"], result[\"generated\"])\n",
    "    rouge1_scores.append(scores[\"rouge1\"].fmeasure)\n",
    "    rouge2_scores.append(scores[\"rouge2\"].fmeasure)\n",
    "    rougeL_scores.append(scores[\"rougeL\"].fmeasure)\n",
    "\n",
    "# Calculate BERTScore\n",
    "generated_texts = [r[\"generated\"] for r in finetuned_results]\n",
    "reference_texts = [r[\"ground_truth\"] for r in finetuned_results]\n",
    "P, R, F1 = bert_score(generated_texts, reference_texts, lang=\"en\", verbose=False)\n",
    "\n",
    "# Store metrics\n",
    "finetuned_metrics = {\n",
    "    \"rouge1\": np.mean(rouge1_scores),\n",
    "    \"rouge2\": np.mean(rouge2_scores),\n",
    "    \"rougeL\": np.mean(rougeL_scores),\n",
    "    \"bertscore_p\": P.mean().item(),\n",
    "    \"bertscore_r\": R.mean().item(),\n",
    "    \"bertscore_f1\": F1.mean().item()\n",
    "}\n",
    "\n",
    "print(f\"\\nROUGE-1: {finetuned_metrics['rouge1']:.4f}\")\n",
    "print(f\"ROUGE-2: {finetuned_metrics['rouge2']:.4f}\")\n",
    "print(f\"ROUGE-L: {finetuned_metrics['rougeL']:.4f}\")\n",
    "print(f\"BERTScore F1: {finetuned_metrics['bertscore_f1']:.4f}\")\n",
    "\n",
    "# Compare to baseline\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"IMPROVEMENT OVER BASELINE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"ROUGE-L: {baseline_metrics['rougeL']:.4f} -> {finetuned_metrics['rougeL']:.4f} ({(finetuned_metrics['rougeL']/baseline_metrics['rougeL']-1)*100:+.1f}%)\")\n",
    "print(f\"BERTScore: {baseline_metrics['bertscore_f1']:.4f} -> {finetuned_metrics['bertscore_f1']:.4f} ({(finetuned_metrics['bertscore_f1']/baseline_metrics['bertscore_f1']-1)*100:+.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8c211e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TESTING: TL;DR PROMPT ON FINE-TUNED MODEL\n",
      "============================================================\n",
      "\n",
      "Fine-tuned + TL;DR ROUGE-L:     0.0842\n",
      "Fine-tuned + Summarize ROUGE-L: 0.1188\n",
      "Baseline (mixed prompts) ROUGE-L: 0.1529\n"
     ]
    }
   ],
   "source": [
    "def generate_summary_finetuned_tldr(text, model, tokenizer, max_new_tokens=150):\n",
    "    \"\"\"Generate summary using fine-tuned model with TL;DR prompt.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Use TL;DR format instead of Summarize\n",
    "    prompt = f\"TL;DR:\\n{text[:1000]}\\n\\nTL;DR:\"\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=800\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            num_beams=4,\n",
    "            early_stopping=True,\n",
    "            no_repeat_ngram_size=3,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=False\n",
    "        )\n",
    "    \n",
    "    full_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    if \"TL;DR:\" in full_output:\n",
    "        summary = full_output.split(\"TL;DR:\")[-1].strip()\n",
    "    else:\n",
    "        summary = full_output[len(prompt):].strip()\n",
    "    \n",
    "    return summary\n",
    "print(\"=\"*60)\n",
    "print(\"TESTING: TL;DR PROMPT ON FINE-TUNED MODEL\")\n",
    "print(\"=\"*60)\n",
    "tldr_finetuned_results = []\n",
    "for idx in sample_indices:\n",
    "    example = dataset[\"test\"][idx]\n",
    "    generated = generate_summary_finetuned_tldr(example[\"text\"], lora_model, tokenizer)\n",
    "    \n",
    "    tldr_finetuned_results.append({\n",
    "        \"index\": idx,\n",
    "        \"ground_truth\": example[\"abstract\"],\n",
    "        \"generated\": generated\n",
    "    })\n",
    "# Evaluate\n",
    "rouge_scores = []\n",
    "for result in tldr_finetuned_results:\n",
    "    scores = scorer.score(result[\"ground_truth\"], result[\"generated\"])\n",
    "    rouge_scores.append(scores[\"rougeL\"].fmeasure)\n",
    "avg_rougeL = np.mean(rouge_scores)\n",
    "print(f\"\\nFine-tuned + TL;DR ROUGE-L:     {avg_rougeL:.4f}\")\n",
    "print(f\"Fine-tuned + Summarize ROUGE-L: {finetuned_metrics['rougeL']:.4f}\")\n",
    "print(f\"Baseline (mixed prompts) ROUGE-L: {baseline_metrics['rougeL']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task9_header",
   "metadata": {},
   "source": [
    "# TASK 9 - Compare & Contrast All Approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "comparison_table",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "COMPREHENSIVE COMPARISON: ALL APPROACHES\n",
      "======================================================================\n",
      "\n",
      "Approach                  ROUGE-1    ROUGE-2    ROUGE-L    BERT-F1   \n",
      "----------------------------------------------------------------------\n",
      "Baseline (TL;DR)          0.2907     0.0437     0.1529     0.8156\n",
      "Prompt Eng (v4_structure) 0.2917     0.0416     0.1539     0.8167\n",
      "LoRA Fine-tuned           0.1938     0.0370     0.1188     0.8289\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "======================================================================\n",
      "SUMMARY\n",
      "======================================================================\n",
      "Prompt engineering improvement: +0.6% ROUGE-L\n",
      "Fine-tuning improvement: -22.3% ROUGE-L\n",
      "\n",
      "Best approach: Prompt Engineering\n"
     ]
    }
   ],
   "source": [
    "# Task 9: Comprehensive Comparison\n",
    "# =================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"COMPREHENSIVE COMPARISON: ALL APPROACHES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get best prompt engineering result\n",
    "best_prompt_version = \"v4_structure\"\n",
    "best_prompt_metrics = prompt_engineering_metrics[best_prompt_version]\n",
    "\n",
    "print(f\"\\n{'Approach':<25} {'ROUGE-1':<10} {'ROUGE-2':<10} {'ROUGE-L':<10} {'BERT-F1':<10}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Baseline\n",
    "print(f\"{'Baseline (TL;DR)':<25} {baseline_metrics['rouge1']:.4f}     {baseline_metrics['rouge2']:.4f}     {baseline_metrics['rougeL']:.4f}     {baseline_metrics['bertscore_f1']:.4f}\")\n",
    "\n",
    "# Best Prompt Engineering\n",
    "print(f\"{'Prompt Eng (v4_structure)':<25} {best_prompt_metrics['rouge1']:.4f}     {best_prompt_metrics['rouge2']:.4f}     {best_prompt_metrics['rougeL']:.4f}     {best_prompt_metrics['bertscore_f1']:.4f}\")\n",
    "\n",
    "# Fine-tuned\n",
    "print(f\"{'LoRA Fine-tuned':<25} {finetuned_metrics['rouge1']:.4f}     {finetuned_metrics['rouge2']:.4f}     {finetuned_metrics['rougeL']:.4f}     {finetuned_metrics['bertscore_f1']:.4f}\")\n",
    "\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Calculate improvements\n",
    "prompt_improvement = (best_prompt_metrics['rougeL'] / baseline_metrics['rougeL'] - 1) * 100\n",
    "finetune_improvement = (finetuned_metrics['rougeL'] / baseline_metrics['rougeL'] - 1) * 100\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"SUMMARY\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Prompt engineering improvement: {prompt_improvement:+.1f}% ROUGE-L\")\n",
    "print(f\"Fine-tuning improvement: {finetune_improvement:+.1f}% ROUGE-L\")\n",
    "print(f\"\\nBest approach: {'Fine-tuned' if finetuned_metrics['rougeL'] > best_prompt_metrics['rougeL'] else 'Prompt Engineering'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
